#ifndef AllActivationFunctions
#define AllActivationFunctions
//ReLU is generally used as the activation function
//for the hidden layers of the network
float ReLU(float x);


float exponentiation(float x);


#endif